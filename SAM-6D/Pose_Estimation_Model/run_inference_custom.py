import gorilla
import argparse
import os
import sys
from PIL import Image
import os.path as osp
import numpy as np
import random
import importlib
import json
import time

import torch
import torchvision.transforms as transforms
import cv2

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, '..', 'Pose_Estimation_Model')
sys.path.append(os.path.join(ROOT_DIR, 'provider'))
sys.path.append(os.path.join(ROOT_DIR, 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))
sys.path.append(os.path.join(BASE_DIR, 'model', 'pointnet2'))

# Convert string inputs to boolean
def str2bool(v):

    print("type(v): ", type(v))
    if isinstance(v, bool):
       return v

    if not isinstance(v, str):
        raise argparse.ArgumentTypeError(f"Expected a string for boolean conversion, got {type(v)}.")

    v = v.lower().strip()
    if v in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        # Raise an error if the input is not a recognizable boolean string
        raise argparse.ArgumentTypeError(f"Boolean value expected (e.g., 'true', 'false', '1', '0'), but received '{v}'.")


def get_parser():
    parser = argparse.ArgumentParser(
        description="Pose Estimation")
    # pem
    parser.add_argument("--gpus",
                        type=str,
                        default="0",
                        help="path to pretrain model")
    parser.add_argument("--model",
                        type=str,
                        default="pose_estimation_model",
                        help="path to model file")
    parser.add_argument("--config",
                        type=str,
                        default="config/base.yaml",
                        help="path to config file, different config.yaml use different config")
    parser.add_argument("--iter",
                        type=int,
                        default=600000,
                        help="epoch num. for testing")
    parser.add_argument("--exp_id",
                        type=int,
                        default=0,
                        help="")
    
    # input
    parser.add_argument("--output_dir", nargs="?", help="Path to root directory of the output")
    parser.add_argument("--cad_path", nargs="?", help="Path to CAD(mm)")
    parser.add_argument("--rgb_path", nargs="?", help="Path to RGB image")
    parser.add_argument("--depth_path", nargs="?", help="Path to Depth image(mm)")
    parser.add_argument("--cam_path", nargs="?", help="Path to camera information")
    parser.add_argument("--seg_path", nargs="?", help="Path to segmentation information(generated by ISM)")
    parser.add_argument("--det_score_thresh", default=0.2, help="The score threshold of detection")
    parser.add_argument("--low_gpu_memory_mode", type=str2bool, nargs='?', const=True, default=False, metavar="BOOLEAN", help="Use less GPU memory. Allows inference on a GPU with at least 8 GB of RAM")
    args_cfg = parser.parse_args()

    return args_cfg

def init():
    args = get_parser()

    DO_DEBUG_SESSION = True

    if DO_DEBUG_SESSION: # Hijack the script arguments
        ROOT_DIR = "/home/joao/source/SAM-6D/SAM-6D"
        CAD_PATH = f"{ROOT_DIR}/Data/Example/obj_000005.ply"    # path to a given cad model(mm)
        RGB_PATH = f"{ROOT_DIR}/Data/Example/rgb.png"           # path to a given RGB image
        DEPTH_PATH = f"{ROOT_DIR}/Data/Example/depth.png"       # path to a given depth map(mm)
        CAMERA_PATH = f"{ROOT_DIR}/Data/Example/camera.json"    # path to given camera intrinsics
        OUTPUT_DIR = f"{ROOT_DIR}/Data/output"         # path to a pre-defined file for saving results
        TEMPLATE_ROOT_DIR = f"{OUTPUT_DIR}/templates"
        LOW_GPU_MEMORY_MODE = True
        args.output_dir = OUTPUT_DIR
        args.cad_path = CAD_PATH
        args.rgb_path = RGB_PATH
        args.depth_path =DEPTH_PATH
        args.cam_path = CAMERA_PATH
        args.seg_path = OUTPUT_DIR + "/sam6d_results/detection_ism.json"
        args.low_gpu_memory_mode = LOW_GPU_MEMORY_MODE
        os.chdir(f"{ROOT_DIR}/Pose_Estimation_Model")

    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    log_dir = osp.join("log", exp_name)

    cfg = gorilla.Config.fromfile(args.config)
    cfg.exp_name = exp_name
    cfg.gpus     = args.gpus
    cfg.model_name = args.model
    cfg.log_dir  = log_dir
    cfg.test_iter = args.iter

    cfg.output_dir = args.output_dir
    cfg.template_output_dir = TEMPLATE_ROOT_DIR  # TODO fix this
    cfg.cad_path = args.cad_path
    cfg.rgb_path = args.rgb_path
    cfg.depth_path = args.depth_path
    cfg.cam_path = args.cam_path
    cfg.seg_path = args.seg_path

    cfg.low_gpu_memory_mode = args.low_gpu_memory_mode
    cfg.det_score_thresh = args.det_score_thresh
    gorilla.utils.set_cuda_visible_devices(gpu_ids = cfg.gpus)

    return  cfg



from data_utils import (
    load_im,
    get_bbox,
    get_point_cloud_from_depth,
    get_resize_rgb_choose,
)
from draw_utils import draw_detections
import pycocotools.mask as cocomask
import trimesh

rgb_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                    std=[0.229, 0.224, 0.225])])

def visualize(rgb, pred_rot, pred_trans, pose_scores, model_points, K, save_path):
    img = draw_detections(rgb, pred_rot, pred_trans, pose_scores, model_points, K, color=(255, 0, 0))
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    prediction = Image.open(save_path)
    
    # concat side by side in PIL
    rgb = Image.fromarray(np.uint8(rgb))
    img = np.array(img)
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))
    concat.paste(prediction, (img.shape[1], 0))
    return concat


def _get_template(path, cfg, tem_index=1):
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    rgb = load_im(rgb_path).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  
    mask = load_im(mask_path).astype(np.uint8) == 255

    bbox = get_bbox(mask)
    y1, y2, x1, x2 = bbox
    mask = mask[y1:y2, x1:x2]

    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    rgb = rgb_transform(np.array(rgb))

    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))[choose, :]

    rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)
    return rgb, rgb_choose, xyz


def get_templates(path, cfg):
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template(path, cfg, i)
        all_tem.append(torch.FloatTensor(tem).unsqueeze(0).cuda())
        all_tem_choose.append(torch.IntTensor(tem_choose).long().unsqueeze(0).cuda())
        all_tem_pts.append(torch.FloatTensor(tem_pts).unsqueeze(0).cuda())
    return all_tem, all_tem_pts, all_tem_choose


class PoseEstimatorModel:
    def __init__(self):
        self.cfg = init()

        random.seed(self.cfg.rd_seed)
        torch.manual_seed(self.cfg.rd_seed)

        # model
        print("=> creating model ...")
        MODEL = importlib.import_module(self.cfg.model_name)
        self.model = MODEL.Net(self.cfg.model, self.cfg.low_gpu_memory_mode)
        
        if self.cfg.low_gpu_memory_mode:
            self.model = self.model.cpu()
        else:
            self.model = self.model.cuda()

        self.model.eval()
        checkpoint = os.path.join(os.path.dirname((os.path.abspath(__file__))), 'checkpoints', 'sam-6d-pem-base.pth')
        gorilla.solver.load_checkpoint(model=self.model, filename=checkpoint)


    def load_model_to_gpu(self):
        if not next(self.model.parameters()).is_cuda:
            self.model = self.model.cuda()


    def unload_model_to_cpu(self):
        if next(self.model.parameters()).is_cuda:
            self.model = self.model.cpu()
    

    def get_templates(self, template_path_for_object, ):
        all_tem, all_tem_pts, all_tem_choose = get_templates(template_path_for_object, self.cfg.test_dataset)
        with torch.no_grad():
            all_tem_pts, all_tem_feat = self.model.feature_extraction.get_obj_feats(all_tem, all_tem_pts, all_tem_choose)

        return all_tem_pts, all_tem_feat


    def get_point_cloud_from_depth(self, whole_depth, K):
        return get_point_cloud_from_depth(whole_depth, K)


    def load_test_data_from_paths(self, rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, mm_to_meters=True):
        dets = []
        with open(seg_path) as f:
            dets_ = json.load(f) # keys: scene_id, image_id, category_id, bbox, score, segmentation
        for det in dets_:
            if det['score'] > det_score_thresh:
                dets.append(det)
        del dets_

        cam_info = json.load(open(cam_path))
        K = np.array(cam_info['cam_K']).reshape(3, 3)

        whole_image = load_im(rgb_path).astype(np.uint8)
        if len(whole_image.shape)==2:
            whole_image = np.concatenate([whole_image[:,:,None], whole_image[:,:,None], whole_image[:,:,None]], axis=2)

        scale = 1000.0 if mm_to_meters else 1.0

        whole_depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / scale
        whole_pts = self.get_point_cloud_from_depth(whole_depth, K)

        mesh = trimesh.load_mesh(cad_path)
        model_points = self.sample_points_from_mesh(mesh) / scale

        return dets, whole_image, whole_depth, whole_pts, K, model_points


    def sample_points_from_mesh(self, mesh):
        model_points = mesh.sample(self.cfg.test_dataset.n_sample_model_point).astype(np.float32)
        return model_points


    def prepare_test_data(self, dets, whole_image, whole_depth, whole_pts, K, model_points):
        cfg = self.cfg.test_dataset
        
        radius = np.max(np.linalg.norm(model_points, axis=1))

        all_rgb = []
        all_cloud = []
        all_rgb_choose = []
        all_score = []
        all_dets = []
        ret_dict = {}
        for inst in dets:
            
            # Compatible with both 'scores' and 'score' keys
            # Reason: We may be passing dets coming from either: 
            #   - in-memory detections created by InstanceSegmentatorModel
            #   - loading Detections from file, which have different set of attributes altogether.
            score = inst.get('scores', inst.get('score')) # Get 'scores', fallback to 'score'
        
            # Prefer the masks already in memory produced by Instance Segmentation Model stage.
            # Only load masks from file otherwise.
            try:
                inst["masks"].shape
                masks_already_in_memory = True
            except KeyError:
                masks_already_in_memory = False

            if masks_already_in_memory:
                mask = inst["masks"].cpu().numpy()
            else: # need to decode RLE streams
                seg = inst['segmentation']

                # mask
                h,w = seg['size']
                try:
                    rle = cocomask.frPyObjects(seg, h, w)
                except:
                    rle = seg
                mask = cocomask.decode(rle)
            
            mask = np.logical_and(mask > 0, whole_depth > 0)
            if np.sum(mask) > 32:
                bbox = get_bbox(mask)
                y1, y2, x1, x2 = bbox
            else:
                continue
            mask = mask[y1:y2, x1:x2]
            choose = mask.astype(np.float32).flatten().nonzero()[0]

            # pts
            cloud = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
            center = np.mean(cloud, axis=0)
            tmp_cloud = cloud - center[None, :]
            flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
            if np.sum(flag) < 4:
                continue
            choose = choose[flag]
            cloud = cloud[flag]

            if len(choose) <= cfg.n_sample_observed_point:
                choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
            else:
                choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
            choose = choose[choose_idx]
            cloud = cloud[choose_idx]

            # rgb
            rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
            if cfg.rgb_mask_flag:
                rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
            rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
            rgb = rgb_transform(np.array(rgb))
            rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

            all_rgb.append(torch.FloatTensor(rgb))
            all_cloud.append(torch.FloatTensor(cloud))
            all_rgb_choose.append(torch.IntTensor(rgb_choose).long())
            all_score.append(score)
            all_dets.append(inst)

        if len(dets) > 0:          
            ret_dict['pts'] = torch.stack(all_cloud).cuda()
            ret_dict['rgb'] = torch.stack(all_rgb).cuda()
            ret_dict['rgb_choose'] = torch.stack(all_rgb_choose).cuda()
            ret_dict['score'] = torch.FloatTensor(all_score).cuda()
        else:
            ret_dict['pts'] = torch.zeros((0,0,0)).cuda()
            ret_dict['rgb'] = torch.zeros((0,0,0,0)).cuda()
            ret_dict['rgb_choose'] = torch.zeros((0,0)).cuda()
            ret_dict['score'] = torch.zeros((0,)).cuda()

        ninstance = ret_dict['pts'].size(0)
        ret_dict['model'] = torch.FloatTensor(model_points).unsqueeze(0).repeat(ninstance, 1, 1).cuda()
        ret_dict['K'] = torch.FloatTensor(K).unsqueeze(0).repeat(ninstance, 1, 1).cuda()
        return ret_dict, all_dets
    

    def infer_pose(self, all_tem_pts, all_tem_feat, input_data, chunk_size=5):
        ninstance = input_data['pts'].size(0)

        # Lists to store results from each chunk before concatenation
        all_pose_scores_list = []
        all_pred_rot_list = []
        all_pred_trans_list = []

        print("=> running model ...")
        with torch.inference_mode():
            for i in range(0, ninstance, chunk_size):
                start_idx = i
                end_idx = min(i + chunk_size, ninstance)
                current_chunk_size = end_idx - start_idx

                # Create a dictionary for the current chunk's data
                chunk_input_data = {}
                for key, value in input_data.items():
                    # Slice tensors that have the batch dimension (ninstance)
                    #if isinstance(value, torch.Tensor) and value.size(0) == ninstance:
                    chunk_input_data[key] = value[start_idx:end_idx]

                chunk_input_data['dense_po'] = all_tem_pts.repeat(current_chunk_size, 1, 1)
                chunk_input_data['dense_fo'] = all_tem_feat.repeat(current_chunk_size, 1, 1)

                #with torch.autocast(device_type='cuda', dtype=torch.float16):
                out_chunk = self.model(chunk_input_data)

                # Process output chunk
                if 'pred_pose_score' in out_chunk.keys():
                    pose_scores_chunk = out_chunk['pred_pose_score'] * out_chunk['score']
                else:
                    pose_scores_chunk = out_chunk['score']
                
                # Detach and move results to CPU incrementally to free GPU memory
                all_pose_scores_list.append(pose_scores_chunk.detach().cpu())
                all_pred_rot_list.append(out_chunk['pred_R'].detach().cpu())
                all_pred_trans_list.append(out_chunk['pred_t'].detach().cpu())

            if ninstance > 0:
                pose_scores = torch.cat(all_pose_scores_list, dim=0).numpy()
                pred_rot = torch.cat(all_pred_rot_list, dim=0).numpy()
                pred_trans = torch.cat(all_pred_trans_list, dim=0).numpy()
            else:
                pose_scores = np.zeros((0,))
                pred_rot = np.zeros((0,3,3))
                pred_trans = np.zeros((0,3))

        return pose_scores,pred_rot,pred_trans


    def render_predictions(self, whole_image, model_points, input_data, pose_scores, pred_rot, pred_trans, save_path, only_highest_score = True, min_score_threshold=0.3, meters2mm=True):
        
        if only_highest_score:
            valid_masks = pose_scores == pose_scores.max() and pose_scores >= min_score_threshold
        else:
            valid_masks = pose_scores >= min_score_threshold
        
        scale = 1000.0 if meters2mm else 1.0
        K = input_data['K'].detach().cpu().numpy()[valid_masks]
        vis_img = visualize(whole_image, pred_rot[valid_masks], pred_trans[valid_masks], pose_scores[valid_masks], model_points*scale, K, save_path)
        vis_img.save(save_path)


if __name__ == "__main__":
    pem = PoseEstimatorModel()

    print("=> extracting templates ...")
    all_tem_pts, all_tem_feat = pem.get_templates(pem.cfg.template_output_dir)

    print("=> loading input data ...")
    
    cfg = pem.cfg

    dets, whole_image, whole_depth, whole_pts, K, model_points = pem.load_test_data_from_paths(
                                                                cfg.rgb_path, cfg.depth_path, cfg.cam_path, cfg.cad_path, 
                                                                cfg.seg_path, cfg.det_score_thresh, mm_to_meters=True)

    input_data, detections = pem.prepare_test_data(dets, whole_image, whole_depth, whole_pts, K, model_points)

    start_time = time.time()
    pose_scores, pred_rot, pred_trans = pem.infer_pose(all_tem_pts, all_tem_feat, input_data)
    pred_trans *= 1000.0 # convert from meters back to millimeters
    print(f"inference time: {time.time() - start_time:0.1f} seconds")

    print("=> saving results ...")
    os.makedirs(f"{cfg.output_dir}/sam6d_results", exist_ok=True)
    for idx, det in enumerate(detections):
        detections[idx]['score'] = float(pose_scores[idx])
        detections[idx]['R'] = list(pred_rot[idx].tolist())
        detections[idx]['t'] = list(pred_trans[idx].tolist())

    with open(os.path.join(f"{cfg.output_dir}/sam6d_results", 'detection_pem.json'), "w") as f:
        json.dump(detections, f)

    print("=> visualizating ...")
    save_path = os.path.join(f"{cfg.output_dir}/sam6d_results", 'vis_pem.png')
    pem.render_predictions(whole_image, model_points, input_data, pose_scores, pred_rot, pred_trans, save_path)

